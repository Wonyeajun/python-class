from datasets import load_dataset, load_from_disk
from transformers import T5Tokenizer, T5ForConditionalGeneration
from evaluate import load
# import torch nltk rouge-score absl sentencepiece
# CNN/Daily Mail 데이터셋 로드
dataset = load_dataset("cnn_dailymail", "3.0.0")

model_dir = "checkpoint19000"
model = T5ForConditionalGeneration.from_pretrained(model_dir)
tokenizer = T5Tokenizer.from_pretrained(model_dir)
model = model.to("cpu")

test_text = dataset["test"][0]["article"]
input_ids = (tokenizer("summarize: " + test_text, return_tensors="pt", truncation=True, max_length=512) .input_ids.to("cpu"))
outputs = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)

rouge = load("rouge")

references = []
predictions = []

example = dataset["test"][0]
article = example["article"]
input_ids = (tokenizer("summarize: " + article, return_tensors="pt", truncation=True, max_length=512)
                 .input_ids.to("cpu"))
outputs = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)
generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
reference_summary = example["highlights"]

predictions.append(generated_summary)
references.append(reference_summary)


results = rouge.compute(predictions=predictions, references=references)
print("\nROUGE 평가 결과:")
for key, value in results.items():
    print(f"{key}: {value}")

# 0번 데이터 보기
example = dataset["test"][0]
article = example["article"]
highlight = example["highlights"]

# 원문 데이터와 요약 데이터 출력
print("원문 기사:")
print(article)
print("\n요약된 기사:")
print(highlight)
print('\n모델 요약')
print(predictions)
